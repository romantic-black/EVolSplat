Metadata-Version: 2.4
Name: nerfstudio
Version: 1.1.2
Summary: All-in-one repository for state-of-the-art NeRFs
License: Apache 2.0
Project-URL: Documentation, https://docs.nerf.studio
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python
Requires-Python: >=3.8.0
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: appdirs>=1.4
Requires-Dist: av>=9.2.0
Requires-Dist: awscli>=1.31.10
Requires-Dist: comet_ml>=3.33.8
Requires-Dist: cryptography>=38
Requires-Dist: tyro>=0.6.6
Requires-Dist: gdown>=4.6.0
Requires-Dist: ninja>=1.10
Requires-Dist: h5py>=2.9.0
Requires-Dist: imageio>=2.21.1
Requires-Dist: importlib-metadata>=6.0.0; python_version < "3.10"
Requires-Dist: ipywidgets>=7.6
Requires-Dist: jaxtyping==0.2.19
Requires-Dist: jupyterlab>=3.3.4
Requires-Dist: matplotlib>=3.6.0
Requires-Dist: mediapy>=1.1.0
Requires-Dist: msgpack>=1.0.4
Requires-Dist: msgpack_numpy>=0.4.8
Requires-Dist: nerfacc==0.5.2
Requires-Dist: open3d>=0.16.0
Requires-Dist: opencv-python==4.8.0.76
Requires-Dist: Pillow>=10.3.0
Requires-Dist: plotly>=5.7.0
Requires-Dist: protobuf!=3.20.0,<=3.20.3
Requires-Dist: pymeshlab>=2022.2.post2; platform_machine != "arm64" and platform_machine != "aarch64"
Requires-Dist: pyngrok>=5.1.0
Requires-Dist: python-socketio>=5.7.1
Requires-Dist: pyquaternion>=0.9.9
Requires-Dist: rawpy>=0.18.1; platform_machine != "arm64"
Requires-Dist: newrawpy>=1.0.0b0; platform_machine == "arm64"
Requires-Dist: requests
Requires-Dist: rich>=12.5.1
Requires-Dist: scikit-image>=0.19.3
Requires-Dist: splines==0.3.0
Requires-Dist: tensorboard>=2.13.0
Requires-Dist: torch>=1.13.1
Requires-Dist: torchvision>=0.14.1
Requires-Dist: torchmetrics[image]>=1.0.1
Requires-Dist: typing_extensions>=4.4.0
Requires-Dist: viser==0.1.27
Requires-Dist: nuscenes-devkit>=1.1.1
Requires-Dist: wandb>=0.13.3
Requires-Dist: xatlas
Requires-Dist: trimesh>=3.20.2
Requires-Dist: timm==0.6.7
Requires-Dist: gsplat==1.0.0
Requires-Dist: pytorch-msssim
Requires-Dist: pathos
Requires-Dist: packaging
Requires-Dist: fpsample
Requires-Dist: omegaconf==2.3.0
Requires-Dist: easydict
Requires-Dist: einops
Requires-Dist: moviepy==1.0.3
Provides-Extra: gen
Requires-Dist: diffusers==0.16.1; extra == "gen"
Requires-Dist: transformers==4.29.2; extra == "gen"
Requires-Dist: accelerate==0.19.0; extra == "gen"
Requires-Dist: bitsandbytes==0.39.0; extra == "gen"
Requires-Dist: sentencepiece==0.1.99; extra == "gen"
Provides-Extra: dev
Requires-Dist: pre-commit==3.3.2; extra == "dev"
Requires-Dist: pytest==7.1.2; extra == "dev"
Requires-Dist: pytest-xdist==2.5.0; extra == "dev"
Requires-Dist: typeguard==2.13.3; extra == "dev"
Requires-Dist: ruff==0.1.13; extra == "dev"
Requires-Dist: sshconf==0.2.5; extra == "dev"
Requires-Dist: pycolmap>=0.3.0; extra == "dev"
Requires-Dist: diffusers==0.16.1; extra == "dev"
Requires-Dist: opencv-stubs==0.0.7; extra == "dev"
Requires-Dist: transformers==4.29.2; extra == "dev"
Requires-Dist: pyright==1.1.331; extra == "dev"
Requires-Dist: projectaria-tools>=1.3.1; sys_platform != "win32" and extra == "dev"
Requires-Dist: torch<2.2,>=1.13.1; extra == "dev"
Provides-Extra: docs
Requires-Dist: furo==2022.09.29; extra == "docs"
Requires-Dist: ipython==8.6.0; extra == "docs"
Requires-Dist: readthedocs-sphinx-search==0.1.2; extra == "docs"
Requires-Dist: myst-nb==0.16.0; extra == "docs"
Requires-Dist: nbconvert==7.2.5; extra == "docs"
Requires-Dist: nbformat==5.9.2; extra == "docs"
Requires-Dist: sphinx==5.2.1; extra == "docs"
Requires-Dist: sphinxemoji==0.2.0; extra == "docs"
Requires-Dist: sphinx-argparse==0.3.1; extra == "docs"
Requires-Dist: sphinx-copybutton==0.5.0; extra == "docs"
Requires-Dist: sphinx-design==0.2.0; extra == "docs"
Requires-Dist: sphinxext-opengraph==0.6.3; extra == "docs"
Dynamic: license-file

<h1 align="center">EVolSplat: Efficient Volumetric Splatting for Real-Time Urban View Synthesis (CVPR 2025)</h1>

[![paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/pdf/2503.20168)

Sheng Miao, [Jiaxin Huang](https://jaceyhuang.github.io/), Dongfeng Bai, Xu Yan, [Hongyu Zhou](https://hyzhou404.github.io/), [Yue Wang](https://ywang-zju.github.io/), Bingbing Liu, [Andreas Geiger](https://www.cvlibs.net/) and [Yiyi Liao](https://yiyiliao.github.io/) 

Our project page can be seen [here](https://xdimlab.github.io/EVolSplat/).
<img src="./docs/teaser.png" height="200">
## :book: Datasets
We evaluate our model on [KITTI-360](http://www.cvlibs.net/datasets/kitti-360/) and [Waymo](https://waymo.com/open/download/). Here we show the structure of a test dataset as follow, similar to the [EDUS](https://xdimlab.github.io/EDUS/). We provide the one example data for inference on KITTI-360, which can be found in huggingface [here](https://huggingface.co/datasets/cookiemiao/EvolSplat/tree/main).


The dataset should have a structure as follows:
```
‚îú‚îÄ‚îÄ $PATH_TO_YOUR_DATASET
    ‚îú‚îÄ‚îÄ $SCENE_0
        ‚îú‚îÄ‚îÄ depth
        ‚îú‚îÄ‚îÄ pointcloud/*.ply
        ‚îú‚îÄ‚îÄ *.png
        ...
        ‚îú‚îÄ‚îÄ transfroms.json
    ...
    ‚îú‚îÄ‚îÄ SCENE_N
        ‚îú‚îÄ‚îÄ depth
        ‚îú‚îÄ‚îÄ pointcloud/*.ply
        ‚îú‚îÄ‚îÄ *.png
        ...
        ‚îú‚îÄ‚îÄ transfroms.json
```

## :house: Installation
Our EVolSplat is built on [nerfstudio](https://github.com/nerfstudio-project/nerfstudio). You can follow the nerfstudio webpage to install our code.  


#### Create environment
We recommend using conda to create a new environment and you can find the detailed environment file in `environment.yml`.
```bash
conda create --name EVolSplat -y python=3.8
conda activate EVolSplat
pip install --upgrade pip
```
#### Dependencies
##### Install PyTorch
Install PyTorch with CUDA (this repo has been tested with CUDA 11.8 with torch 2.1.2 and CUDA 11.7 with torch 2.0.1).

For CUDA 11.8 with torch 2.1.2:
```bash
pip install torch==2.1.2+cu118 torchvision==0.16.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
conda install -c "nvidia/label/cuda-11.8.0" cuda-toolkit
```

For CUDA 11.7 with torch 2.0.1:
```bash
pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
conda install -c "nvidia/label/cuda-11.7.1" cuda-toolkit
```
##### Install TorchSparse
Install the spaseCNN library, we recommend the version 2.1.0. We can find the installation instruction in the [torchsparse](https://github.com/mit-han-lab/torchsparse) repository.
```bash
conda install -c conda-forge sparsehash
sudo apt-get install libsparsehash-dev  
git clone --recursive https://github.com/mit-han-lab/torchsparse
python setup.py install
```
##### Install gsplat
Install [gsplat](https://github.com/nerfstudio-project/gsplat) from source code. We recommend the version >= 1.0.0.
```bash
pip install ninja==1.11.1.1
pip install git+https://github.com/nerfstudio-project/gsplat.git
```
If you meet some problems, you can check the dependencies from `environment.yml`.
#### Install EVolSplat
Install EVolSplat form source code
```bash
git clone https://github.com/XDimLab/EVolSplat.git
cd EVolSplat
pip install --upgrade pip setuptools
pip install -e .
```




## :chart_with_upwards_trend: Evaluation & Checkpoint
We provide the pretrained model trained on `KITTI-360`, you can download the pre-trained models and example data from  [huggingface](https://huggingface.co/datasets/cookiemiao/EvolSplat/tree/main) for a quick start. 

Place the downloaded checkpoints in `checkpoints` folder in order to test it later.

### ‚úàÔ∏è Feed-forward Inference
Replace `$PATH_TO_YOUR_DATASET$` with your data path.
```
python nerfstudio/scripts/infer_zeroshot.py evolsplat \
  --load_dir checkpoints/ \
  --pipeline.model.freeze_volume=True \
  zeronpt-data \
  --data $PATH_TO_YOUR_DATASET$ \
  --kitti=True 
```

##  Training
Generate the training sequences by runing `preprocess/run.py`. 
After preparing the training dataset, we provide the training script to train multiple scenes from scratch. The default sparsity is `Drop50`.
```
ns-train evolsplat --data `$PATH_TO_YOUR_DATASET$`
```
## üì¢ News
* 2025-02-27: Accepted by CVPR 2025 !

* 2025-03-26: Project Page and arxiv.

* 2025-06-06: Release dataset preprocess & training code !

## :clipboard: Citation

If our work is useful for your research, please give me a star and consider citing:

```
@inproceedings{miao2025efficient,
  title={EVolSplat: Efficient Volumetric Splatting for Real-Time Urban View Synthesis},
  author={Miao, Sheng and Huang, Jiaxin and Bai, Dongfeng and Yan, Xu and Zhou, Hongyu and Wang, Yue and Liu, Bingbing and Geiger, Andreas and Liao, Yiyi},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
}
```
## :sparkles: Acknowledgement
- This project is based on [nerfstudio](https://github.com/nerfstudio-project/nerfstudio) and [gsplat](https://github.com/nerfstudio-project/gsplat).
- Some codes are brought from [IBRNet](https://github.com/googleinterns/IBRNet) and [SparseNeus](https://github.com/xxlong0/SparseNeuS).
